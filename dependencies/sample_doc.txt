Generative Adversarial Networks, or GANs for short, are a subset of generative modeling. The original idea was outlined by Goodfellow et al. (who have become legends and rockstars in the AI community in their own right) in their 2014 paper introducing GANs.
In a GAN architecture, we have two models: a generator and a discriminator. It is the work of the generator to take some random noise and try and generate realistic samples from it. At the same time, it is the work of the discriminator to look at these samples (intermixed with some real samples from existing data) and distinguish between the two. We will intermittently show the discriminator some generated samples and some real samples, and its work is to tell us the probability that any sample given is real.
A GAN is called adversarial for this reason; two models are competing against each other. The generator is continually trying to develop better, more realistic samples (which are hopefully indistinguishable from the real ones) to try and fool the discriminator. In contrast, the discriminator tries to tell fake, generated samples from real ones successfully.
Generator
Note: the words training and real data are used interchangeably to refer to real pre-existing data samples. The words fake and generated data are also used interchangeably to refer to the samples created by our generator by passing in some noise and operating on it.
A neural network G(z, θ₁) can be used to model the generator, where G takes some random noise z and generates some samples. These samples are then accepted or rejected by the discriminator as real/fake and in this process, G learns a little more about what the real data’s distribution looks like, and subsequently tries to map the random noise it is fed to this training data distribution (i.e., the real data) x.
One can think of it as follows: the training data has some well-defined distribution, while the noise has some non-sensical distribution initially, and over time we tweak the parameters θ₁ of our G such that this random distribution is mapped to mimic the training data until it almost entirely overlaps with (i.e., maps to) it.
To put it simply, generator G’s role is to map the noise z to the desired data space x, and our goal is to learn the parameters θ₁ that will result in the correct mapping of the real distribution.
Discriminator
Conversely, a second neural network D(x, θ₂) estimates the probability that a given sample is real, i.e., the probability that it came from the training data.
Optimization Objectives
Our objective concerning optimizing the discriminator is as follows:
When we have a real sample (remember, x), we speak of D(x). Therefore, we want to maximize this probability D(x). In essence, we want to maximize the probability that the discriminator outputs when it is fed a real sample.
When we have a generated sample (remember, generated from noise z), then we speak of D(G(z)): as in the output of the discriminator on random noise passed through a generator. We wish to optimize the discriminator to minimize D(G(z)) and, hence, correctly classify a fake sample as not real.
Our objective concerning optimizing the generator is as follows:
We want our generator to generate data so realistic that D(G(z)) is 1, i.e., the discriminator is fooled into believing a fake, generated sample is REAL! Therefore, we will optimize to maximize D(G(z)) for training our generator.
The Game of Minimax
At this point, the idea of adversaries kicks in as the generator and discriminator try to optimize a value function in the opposite directions. The generator tries to make the discriminator guess wrongly (by generating realistic samples), and the discriminator tries continuously not to be fooled.
This push-and-pull or tug-of-war or sorts is called a Minimax Game in Game Theory and Computer Science. We do not need to bother ourselves with the minute details, so the essence is simply this: ours is a zero-sum game, i.e., one where the good performance of the generator is tied to the discriminator’s poor performance (and vice versa).
Therefore, there has to be some equilibrium where the two eventually end up ensuring the best possible performance by both the models without entirely jeopardizing each other.
